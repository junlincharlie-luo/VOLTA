Error code: 404 - [{'error': {'code': 404, 'message': 'models/gemini-3.0 is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]
Traceback (most recent call last):
  File "/Users/carrot/Desktop/POPPER/run_all_20_hypotheses.py", line 244, in run_single_hypothesis
    log, last_message, parsed_result = agent.go(hypothesis_data["hypothesis"])
                                       ~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/Users/carrot/Desktop/POPPER/popper/agent.py", line 1040, in go
    for s in self.graph.stream({"messages": ("user", prompt)}, stream_mode="values", config = config):
             ~~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/pregel/__init__.py", line 1315, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        loop.tasks.values(),
        ^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        get_waiter=get_waiter,
        ^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/pregel/runner.py", line 56, in tick
    run_with_retry(t, retry_policy)
    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/pregel/retry.py", line 29, in run_with_retry
    task.proc.invoke(task.input, config)
    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/utils/runnable.py", line 410, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/utils/runnable.py", line 184, in invoke
    ret = context.run(self.func, input, **kwargs)
  File "/Users/carrot/Desktop/POPPER/popper/agent.py", line 925, in design_falsification_test
    proposal = self.test_proposal_agent.go(self.main_hypothesis, test_results, self.log)
  File "/Users/carrot/Desktop/POPPER/popper/agent.py", line 789, in go
    for s in self.app.stream(inputs, stream_mode="values", config = config):
             ~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/pregel/__init__.py", line 1315, in stream
    for _ in runner.tick(
             ~~~~~~~~~~~^
        loop.tasks.values(),
        ^^^^^^^^^^^^^^^^^^^^
    ...<2 lines>...
        get_waiter=get_waiter,
        ^^^^^^^^^^^^^^^^^^^^^^
    ):
    ^
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/pregel/runner.py", line 56, in tick
    run_with_retry(t, retry_policy)
    ~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/pregel/retry.py", line 29, in run_with_retry
    task.proc.invoke(task.input, config)
    ~~~~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/utils/runnable.py", line 410, in invoke
    input = context.run(step.invoke, input, config, **kwargs)
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/utils/runnable.py", line 176, in invoke
    ret = context.run(self.func, input, **kwargs)
  File "/opt/miniconda3/lib/python3.13/site-packages/langgraph/prebuilt/chat_agent_executor.py", line 533, in call_model
    response = model_runnable.invoke(state, config)
  File "/opt/miniconda3/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 3024, in invoke
    input = context.run(step.invoke, input, config)
  File "/opt/miniconda3/lib/python3.13/site-packages/langchain_core/runnables/base.py", line 5354, in invoke
    return self.bound.invoke(
           ~~~~~~~~~~~~~~~~~^
        input,
        ^^^^^^
        self._merge_configs(config),
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
        **{**self.kwargs, **kwargs},
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/opt/miniconda3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 286, in invoke
    self.generate_prompt(
    ~~~~~~~~~~~~~~~~~~~~^
        [self._convert_input(input)],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    ...<6 lines>...
        **kwargs,
        ^^^^^^^^^
    ).generations[0][0],
    ^
  File "/opt/miniconda3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 786, in generate_prompt
    return self.generate(prompt_messages, stop=stop, callbacks=callbacks, **kwargs)
           ~~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 643, in generate
    raise e
  File "/opt/miniconda3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 633, in generate
    self._generate_with_cache(
    ~~~~~~~~~~~~~~~~~~~~~~~~~^
        m,
        ^^
    ...<2 lines>...
        **kwargs,
        ^^^^^^^^^
    )
    ^
  File "/opt/miniconda3/lib/python3.13/site-packages/langchain_core/language_models/chat_models.py", line 851, in _generate_with_cache
    result = self._generate(
        messages, stop=stop, run_manager=run_manager, **kwargs
    )
  File "/opt/miniconda3/lib/python3.13/site-packages/langchain_openai/chat_models/base.py", line 689, in _generate
    response = self.client.create(**payload)
  File "/opt/miniconda3/lib/python3.13/site-packages/openai/_utils/_utils.py", line 286, in wrapper
    return func(*args, **kwargs)
  File "/opt/miniconda3/lib/python3.13/site-packages/openai/resources/chat/completions/completions.py", line 1147, in create
    return self._post(
           ~~~~~~~~~~^
        "/chat/completions",
        ^^^^^^^^^^^^^^^^^^^^
    ...<46 lines>...
        stream_cls=Stream[ChatCompletionChunk],
        ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
    )
    ^
  File "/opt/miniconda3/lib/python3.13/site-packages/openai/_base_client.py", line 1259, in post
    return cast(ResponseT, self.request(cast_to, opts, stream=stream, stream_cls=stream_cls))
                           ~~~~~~~~~~~~^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
  File "/opt/miniconda3/lib/python3.13/site-packages/openai/_base_client.py", line 1047, in request
    raise self._make_status_error_from_response(err.response) from None
openai.NotFoundError: Error code: 404 - [{'error': {'code': 404, 'message': 'models/gemini-3.0 is not found for API version v1main, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.', 'status': 'NOT_FOUND'}}]
